{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d26e803",
   "metadata": {},
   "source": [
    "## This notebook walks you through an example of running a synthetic localizer experiment. You will:\n",
    "1. Load a pretrained brain-optimized model trained on NSD subjects to predict the entire brain\n",
    "2. Run batch inference on the model for all NSD subjects on the popular fLoc experiment (Stigliani et al., 2015) http://vpnl.stanford.edu/fLoc \n",
    "3. Compute word, face, body, and place contrasts on the predicted whole brain responses and record the precision-recall curve metrics\n",
    "4. Define a binary contrast per subject based on max F1 score\n",
    "5. Visualize the model predicted contrast with the ground truth contrast on a flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from nilearn import plotting\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import hcp_utils as hcp\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import mosaic\n",
    "from mosaic.utils.inference import MosaicInference\n",
    "from mosaic.models.transforms import SelectROIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be4845",
   "metadata": {},
   "source": [
    "### 1. Load a pretrained brain-optimized model trained on NSD subjects to predict the entire brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50271fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model, model_config = mosaic.from_pretrained(\n",
    "    backbone_name=\"CNN8\",\n",
    "    vertices=\"all\",\n",
    "    framework=\"multihead\",\n",
    "    subjects=\"NSD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec204433",
   "metadata": {},
   "source": [
    "### 2. Run batch inference on the model for all NSD subjects on the popular fLoc experiment (Stigliani et al., 2015) http://vpnl.stanford.edu/fLoc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the fLoc images from the internet\n",
    "if not os.path.exists(\"./fLoc_stimuli\"):\n",
    "    !wget http://vpnl.stanford.edu/fLoc/fLoc_stimuli.zip\n",
    "    !unzip fLoc_stimuli.zip\n",
    "    !rm fLoc_stimuli.zip\n",
    "else:\n",
    "    print(\"./fLoc_stimuli folder already exists\")\n",
    "stimulus_paths = glob.glob(\"./fLoc_stimuli/*jpg\")\n",
    "print(f\"Found {len(stimulus_paths)} fLoc stimuli (should be 1584)\")\n",
    "images = [Image.open(image_path).convert(\"RGB\") for image_path in stimulus_paths] #convert to PIL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by defualt the model uses cuda if available, otherwise cpu\n",
    "inference = MosaicInference(\n",
    "    model=model,\n",
    "    batch_size=32,\n",
    "    model_config=model_config\n",
    ")\n",
    "\n",
    "results = inference.run(\n",
    "    images = images,\n",
    "    names_and_subjects={\"NSD\": \"all\"}\n",
    ")\n",
    "\n",
    "#reformat output by filename\n",
    "print(\"Reorganizing predictions by filename\")\n",
    "results_by_filename = defaultdict(dict)\n",
    "for dataset in results.keys():\n",
    "    for subjectID, prediction in results[dataset].items():\n",
    "        for idx in range(prediction.shape[0]):\n",
    "            results_by_filename[f\"{subjectID}_{dataset}\"][Path(stimulus_paths[idx]).name] = prediction[idx,:].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434bcdf3",
   "metadata": {},
   "source": [
    "### 3. Compute word, face, body, and place contrasts on the predicted whole brain responses and record the precision-recall curve metrics\n",
    "We are comparing the contrasts synthetically generated from the model with the ground truth ROI class defined in the original NSD experiment. The resulting PR curve is essentially the result of binary classifications at different thresholds. In other words, we say \"if we threshold the contrast at X, these indices are the proposed ROI class (1) and the other indices are not (0)\". Repeating this for many thresholds allows us to construct a PR curve. This allows us to evaluate the synthetic roi predictions agnostic to choosing a (usually arbitrary) threshold. We also compute cutoff values for each contrast as the threshold that gives the max F1 score to be later used as the \"best\" threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrast(activations):\n",
    "    \"\"\"\n",
    "    Compute contrasts according to grill-spector floc. While we can compute it for objects and objects LO those\n",
    "    parts are commented out to match NSD roi classes.\n",
    "    http://vpnl.stanford.edu/fLoc/\n",
    "\n",
    "    INPUTS:\n",
    "    - activations: dict, keys are filenames (str) of the floc image and values are model predictions for that image (float). \n",
    "                    The shape of the model predictions is (nvertices,), where nvertices is the number of vertices the \n",
    "                    model was trained on.\n",
    "    OUTPUTS:\n",
    "    - contrasts: dict, a dictionary where keys are the contrast names and the value is the result (float) of the \n",
    "                    contrast. See 'use_cohensd' for more info on the result.\n",
    "    \"\"\"\n",
    "    categories = [\"body\", \"limb\", \"child\",\"adult\", \"corridor\", \"house\", \"car\", \"instrument\", \"word\", \"number\", \"scrambled\"]\n",
    "    category_dict = {cat: [] for cat in categories}\n",
    "    for filename, activation in activations.items():\n",
    "        cat = filename.split('-')[0]\n",
    "        category_dict[cat].append(activation.flatten())\n",
    "    avg_activation = {cat: np.mean(np.vstack(act), axis=0) for cat, act in category_dict.items()}\n",
    "    contrasts = {'words': [], 'bodies': [], 'faces': [], 'places': []}\n",
    "    for con_name in contrasts.keys():\n",
    "        if con_name == 'words':\n",
    "            pos = ['word', 'number']\n",
    "            neg = [\"body\", \"limb\", \"child\",\"adult\", \"corridor\", \"house\", \"car\", \"instrument\"]\n",
    "        elif con_name == 'bodies':\n",
    "            pos = [\"body\", \"limb\"]\n",
    "            neg = [\"child\",\"adult\" ,\"corridor\", \"house\", \"car\", \"instrument\", \"word\", \"number\"]\n",
    "        elif con_name == 'faces':\n",
    "            pos = [\"child\", \"adult\"]\n",
    "            neg = [\"word\", \"number\", \"body\", \"limb\", \"corridor\", \"house\", \"car\", \"instrument\"]\n",
    "        elif con_name == 'places':\n",
    "            pos = [\"corridor\", \"house\"]\n",
    "            neg = [\"word\", \"number\", \"body\", \"limb\", \"child\", \"adult\", \"car\", \"instrument\"]\n",
    "        else:\n",
    "            raise ValueError(f\"contrast name {con_name} not recognized.\")\n",
    "    \n",
    "        positive_activation = sum([avg_activation[label] for label in pos])/len(pos) \n",
    "        negative_activation = sum([avg_activation[label] for label in neg])/len(neg)\n",
    "        results = positive_activation - negative_activation\n",
    "\n",
    "        contrasts[con_name] = results\n",
    "    return contrasts\n",
    "\n",
    "#Compute precision - recall curve\n",
    "def compute_pr_curve(contrast_values, truth_froi_indices, n_vertices=91282):\n",
    "    \"\"\"\n",
    "    Compute precision-recall curve for ROI prediction across thresholds.\n",
    "    \n",
    "    Args:\n",
    "        contrast_values: array of contrast values for all vertices\n",
    "        truth_froi_indices: set/array of ground truth ROI vertex indices\n",
    "        n_vertices: total number of vertices in brain surface\n",
    "    \n",
    "    Returns:\n",
    "        precision, recall, thresholds arrays\n",
    "    \"\"\"\n",
    "    # Create binary ground truth array\n",
    "    y_true = np.zeros(n_vertices, dtype=bool)\n",
    "    y_true[list(truth_froi_indices)] = True\n",
    "    \n",
    "    # Use contrast values as \"scores\" (higher = more likely to be in ROI)\n",
    "    y_scores = contrast_values\n",
    "    \n",
    "    # Compute PR curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    auc_pr = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    return precision, recall, thresholds, auc_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the contrasts\n",
    "contrast_names = ['words','bodies', 'faces', 'places']\n",
    "subject_contrasts = {subjectID: defaultdict(list) for subjectID in results_by_filename.keys()}\n",
    "for subject in results_by_filename.keys():\n",
    "    subject_contrasts[subject] = compute_contrast(results_by_filename[subject])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca984f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download NSD rois resampled to fsLR32k space\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Download entire folder recursively\n",
    "s3_folder = 'mosaicfmri/assets/nsd_rois/'\n",
    "local_folder = './nsd_rois'\n",
    "if not os.path.exists(os.path.join(f\"{local_folder}/nsd_rois\")):\n",
    "    print(\"Downloading nsd_rois folder from s3 bucket\")\n",
    "    fs.get(s3_folder, local_folder, recursive=True)\n",
    "else:\n",
    "    print(\"nsd_rois folder already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9957ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute PR stats against ground truth\n",
    "if model_config['vertices'] == 'visual':\n",
    "    rois = [f\"GlasserGroup_{x}\" for x in range(1, 6)]\n",
    "elif model_config['vertices'] == 'all':\n",
    "    rois = [f\"GlasserGroup_{x}\" for x in range(1, 23)]\n",
    "ROI_selection = SelectROIs(selected_rois=rois)\n",
    "\n",
    "subject_list = list(subject_contrasts.keys())\n",
    "contrasts_list = list(subject_contrasts[subject_list[0]].keys())\n",
    "cutoffs = {subject: {contrast_name: 0 for contrast_name in contrasts_list} for subject in subject_list} #keep track of best cutoffs\n",
    "pr_results = {contrast_name: {subject_truth: {subject_predict: {} for subject_predict in subject_list} for subject_truth in subject_list} for contrast_name in contrasts_list}\n",
    "\n",
    "for contrast_name in contrasts_list:\n",
    "    for i in range(len(subject_list)):\n",
    "        subA = subject_list[i]\n",
    "        lh_truth = np.load(os.path.join(\"./nsd_rois\", subA.split('_NSD')[0], \"roi_masks\", f\"lh.floc-{contrast_name}_fsLR32k_space_resampled.npy\"))\n",
    "        rh_truth = np.load(os.path.join(\"./nsd_rois\", subA.split('_NSD')[0], \"roi_masks\", f\"rh.floc-{contrast_name}_fsLR32k_space_resampled.npy\"))\n",
    "        ground_truth_data = np.hstack((lh_truth[hcp.vertex_info.grayl], rh_truth[hcp.vertex_info.grayr])) #go from full mesh to defined grayordinates\n",
    "        ground_truth_data_select = ground_truth_data[ROI_selection.selected_roi_indices] #go from defined grayordinates to our selected ROIs (whole brain without some nan indices)\n",
    "        \n",
    "        ground_truth_froi_indices = np.where(ground_truth_data_select > 0)[0]\n",
    "        random_auc_pr = len(ground_truth_froi_indices) / len(ROI_selection.selected_roi_indices) #AUC_pr of a random binary classifier is proportion of positive examples to all examples (p/(p+n))\n",
    "\n",
    "        for j in range(len(subject_list)): #we want the diagonal\n",
    "            subB = subject_list[j]\n",
    "            contrast_values = subject_contrasts[subB][contrast_name]\n",
    "            precision, recall, thresholds, auc_pr = compute_pr_curve(contrast_values, ground_truth_froi_indices, n_vertices=contrast_values.shape[0])\n",
    "\n",
    "            f1 = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-6)\n",
    "            max_f1_idx = np.argmax(f1)\n",
    "            cutoffs[subA][contrast_name] = thresholds[max_f1_idx]\n",
    "                \n",
    "            pr_results[contrast_name][subA][subB] = {\n",
    "                 'precision': precision,\n",
    "                'recall': recall, \n",
    "                'thresholds': thresholds,\n",
    "                'auc_pr': auc_pr,\n",
    "                'random_auc_pr': random_auc_pr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da055164",
   "metadata": {},
   "source": [
    "### 4. Define a binary contrast per subject based on max F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83932635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold contrast by maximum F1 score\n",
    "subject_roi_predictions = {subjectID: defaultdict(list) for subjectID in results_by_filename.keys()}\n",
    "allsubject_roi_predictions = {con_name: np.zeros((len(ROI_selection.selected_roi_indices),)) for con_name in contrast_names}\n",
    "for subject in results_by_filename.keys():\n",
    "    contrasts = subject_contrasts[subject]\n",
    "    for con_name, stat in contrasts.items():\n",
    "        mask = np.zeros_like(stat)\n",
    "        #for f1 cutoff\n",
    "        cutoff = cutoffs[subject][con_name]\n",
    "        top_k_indices = np.where(stat > cutoff)[0]\n",
    "\n",
    "        mask[top_k_indices] = 1\n",
    "        subject_roi_predictions[subject][con_name] = list(np.argwhere(mask).squeeze())\n",
    "        allsubject_roi_predictions[con_name] += mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9ef7e",
   "metadata": {},
   "source": [
    "### 5. Visualize the model predicted contrast with the ground truth contrast on a flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flatmap(stat, title=\"\", cmap='hot', cmap_flag=False, save_flag=False):\n",
    "    #Save flat maps. hemispheres are combined in one plot\n",
    "    #get the data for both hemispheres\n",
    "    cortex_data_left = hcp.left_cortex_data(stat)\n",
    "    cortex_data_right = hcp.right_cortex_data(stat)\n",
    "\n",
    "    #determine global min/max for consistent color scaling\n",
    "    datamin = min(np.nanmin(cortex_data_left), np.nanmin(cortex_data_right))\n",
    "    datamax = max(np.nanmax(cortex_data_left), np.nanmax(cortex_data_right))\n",
    "    vmin=None #datamin\n",
    "    vmax=None\n",
    "    threshold = None\n",
    "    #create a figure with multiple axes to plot each anatomical image\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), subplot_kw={'projection': '3d'})\n",
    "    plt.subplots_adjust(wspace=-.4)\n",
    "    im = plotting.plot_surf(hcp.mesh.flat_left, cortex_data_left,\n",
    "            threshold=threshold, bg_map=hcp.mesh.sulc_left, vmin=vmin, vmax=vmax,\n",
    "            colorbar=cmap_flag, cmap=cmap,\n",
    "            axes = axes[0])\n",
    "    im = plotting.plot_surf(hcp.mesh.flat_right, cortex_data_right,\n",
    "            threshold=threshold, bg_map=hcp.mesh.sulc_right, vmin=vmin, vmax=vmax,\n",
    "            colorbar=cmap_flag, cmap=cmap,\n",
    "            axes = axes[1])\n",
    "    \n",
    "    #flip along the horizontal\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[1].invert_yaxis()\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    if save_flag:\n",
    "        if not title:\n",
    "            print(\"Warning: title is blank, so saved output filenames may overwrite one another.\")\n",
    "        plt.savefig(f\"{title}_flatmap.svg\")\n",
    "        plt.savefig(f\"{title}_flatmap.png\", dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac06a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare single subject ROI class predictions with ground truth\n",
    "comparison_subject = 'sub-01_NSD' ### change me\n",
    "for con_name in contrast_names:\n",
    "    mask = np.zeros((len(ROI_selection.selected_roi_indices),))\n",
    "    mask[subject_roi_predictions[comparison_subject][con_name]] = 1\n",
    "    wb_predictions = ROI_selection.sample2wb(mask) #go to whole brain\n",
    "    wb_predictions[wb_predictions == 0] = np.nan\n",
    "    wb_predictions[wb_predictions > 0] = 100\n",
    "    plot_flatmap(wb_predictions, title=f\"{comparison_subject} subject {con_name} Prediction\", cmap='hot_r')\n",
    "\n",
    "    lh_truth = np.load(os.path.join(\"./nsd_rois\", comparison_subject.split('_NSD')[0], \"roi_masks\", f\"lh.floc-{con_name}_fsLR32k_space_resampled.npy\"))\n",
    "    rh_truth = np.load(os.path.join(\"./nsd_rois\", comparison_subject.split('_NSD')[0], \"roi_masks\", f\"rh.floc-{con_name}_fsLR32k_space_resampled.npy\"))\n",
    "    ground_truth_data = np.hstack((lh_truth[hcp.vertex_info.grayl], rh_truth[hcp.vertex_info.grayr]))\n",
    "\n",
    "    ground_truth_data[ground_truth_data == 0] = np.nan\n",
    "    ground_truth_data[ground_truth_data > 0] = 100 #\n",
    "    plot_flatmap(ground_truth_data, title=f\"{comparison_subject} subject {con_name} Truth\", cmap='hot_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0563ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaic-dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
